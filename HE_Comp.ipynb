{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HE Comp.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeepManuPy/MultiLayer-Perceptrons/blob/master/HE_Comp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "odAVvjisFAXu",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6bVOi9-Z7Vz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "seed = 7\n",
        "np.random.seed(seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyxm65-hEIcX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"train.csv\")\n",
        "\n",
        "dataset = df.values\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "to_drop = ['ID','DateTime','HiDir','WindDir']\n",
        "df = df.drop(columns = to_drop)\n",
        "\n",
        "feature_cols = ['TempOut','HiTemp','LowTemp','OutHum','DewPt','WindSpeed',\n",
        "               'WindRun','HiSpeed','WindChill','HeatIndex','THWIndex','Bar',\n",
        "               'Rain','RainRate','HeatDD','CoolDD','InTemp','InDew','InDew',\n",
        "               'InHeat','InEMC','InAirDensity','WindSamp','WindTx','ISSRecpt',\n",
        "               'ArcInt']\n",
        "target_cols = ['PA','PB','PC','PD','PE','PF','PG']\n",
        "\n",
        "X = df[feature_cols].values\n",
        "y = df[target_cols].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpLEr4-tMWw_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scale = StandardScaler()\n",
        "X[] = scale.fit_transform(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTEmO6Sxki1b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "def root_mean_squared_error(y_true,y_pred):\n",
        "  return K.sqrt(K.mean(K.square(y_pred - y_true),axis=-1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcHcrL0xakGK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def baseline_model():\n",
        "  model = Sequential()\n",
        "  model.add(Dense(26,input_dim=26,activation='relu'))\n",
        "  model.add(Dense(13,activation='relu'))\n",
        "  model.add(Dense(7,activation='linear'))\n",
        "  model.compile(optimizer='adam',loss=root_mean_squared_error,metrics=['accuracy'])\n",
        "  return model\n",
        "def model_1():\n",
        "  model = Sequential()\n",
        "  model.add(Dense(26,input_dim=26,activation='relu'))\n",
        "  model.add(Dense(18,activation='relu'))\n",
        "  model.add(Dense(7,activation='linear'))\n",
        "  model.compile(optimizer='adam',loss=root_mean_squared_error,metrics=['accuracy'])\n",
        "  return model\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAjt1UQBan-r",
        "colab_type": "code",
        "outputId": "5a941db4-4bee-4b89-b014-c988771d731d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6984
        }
      },
      "source": [
        "model = model_1()\n",
        "\n",
        "model.fit(X,y,epochs=200,batch_size=16,validation_split=0.1,verbose=1)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 36000 samples, validate on 4000 samples\n",
            "Epoch 1/200\n",
            "36000/36000 [==============================] - 4s 101us/sample - loss: 162.8096 - acc: 0.9886 - val_loss: 155.6426 - val_acc: 0.9998\n",
            "Epoch 2/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 159.9343 - acc: 0.9825 - val_loss: 154.6039 - val_acc: 0.9540\n",
            "Epoch 3/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 159.0030 - acc: 0.9720 - val_loss: 154.6341 - val_acc: 0.8727\n",
            "Epoch 4/200\n",
            "36000/36000 [==============================] - 3s 88us/sample - loss: 157.9844 - acc: 0.9644 - val_loss: 152.3745 - val_acc: 0.9650\n",
            "Epoch 5/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 156.8801 - acc: 0.9621 - val_loss: 151.6519 - val_acc: 0.9525\n",
            "Epoch 6/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 155.9533 - acc: 0.9580 - val_loss: 151.2964 - val_acc: 0.9178\n",
            "Epoch 7/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 155.0836 - acc: 0.9551 - val_loss: 149.9881 - val_acc: 0.9553\n",
            "Epoch 8/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 154.3154 - acc: 0.9547 - val_loss: 150.9239 - val_acc: 0.9895\n",
            "Epoch 9/200\n",
            "36000/36000 [==============================] - 3s 91us/sample - loss: 153.4799 - acc: 0.9556 - val_loss: 148.7571 - val_acc: 0.9488\n",
            "Epoch 10/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 153.0472 - acc: 0.9544 - val_loss: 148.6876 - val_acc: 0.9140\n",
            "Epoch 11/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 152.4813 - acc: 0.9546 - val_loss: 148.4787 - val_acc: 0.9538\n",
            "Epoch 12/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 152.1749 - acc: 0.9563 - val_loss: 148.0718 - val_acc: 0.9090\n",
            "Epoch 13/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 151.9158 - acc: 0.9577 - val_loss: 147.5767 - val_acc: 0.9800\n",
            "Epoch 14/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 151.2711 - acc: 0.9607 - val_loss: 147.0418 - val_acc: 0.9753\n",
            "Epoch 15/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 150.9666 - acc: 0.9588 - val_loss: 146.8024 - val_acc: 0.9678\n",
            "Epoch 16/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 150.6519 - acc: 0.9596 - val_loss: 147.3476 - val_acc: 0.9890\n",
            "Epoch 17/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 150.2105 - acc: 0.9606 - val_loss: 146.4363 - val_acc: 0.9810\n",
            "Epoch 18/200\n",
            "36000/36000 [==============================] - 3s 88us/sample - loss: 149.9311 - acc: 0.9595 - val_loss: 146.5569 - val_acc: 0.9758\n",
            "Epoch 19/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 150.0697 - acc: 0.9602 - val_loss: 146.6620 - val_acc: 0.9758\n",
            "Epoch 20/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 149.5160 - acc: 0.9591 - val_loss: 148.1720 - val_acc: 0.9663\n",
            "Epoch 21/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 149.3501 - acc: 0.9604 - val_loss: 145.7860 - val_acc: 0.9302\n",
            "Epoch 22/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 149.1388 - acc: 0.9589 - val_loss: 145.3161 - val_acc: 0.9675\n",
            "Epoch 23/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 149.0474 - acc: 0.9613 - val_loss: 147.4171 - val_acc: 0.9465\n",
            "Epoch 24/200\n",
            "36000/36000 [==============================] - 3s 88us/sample - loss: 149.1694 - acc: 0.9600 - val_loss: 144.9006 - val_acc: 0.9168\n",
            "Epoch 25/200\n",
            "36000/36000 [==============================] - 3s 88us/sample - loss: 148.4005 - acc: 0.9626 - val_loss: 146.9878 - val_acc: 0.9893\n",
            "Epoch 26/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 148.5687 - acc: 0.9656 - val_loss: 145.9859 - val_acc: 0.9910\n",
            "Epoch 27/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 148.3323 - acc: 0.9644 - val_loss: 146.0300 - val_acc: 0.9990\n",
            "Epoch 28/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 148.2956 - acc: 0.9631 - val_loss: 144.6917 - val_acc: 0.9783\n",
            "Epoch 29/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 148.2181 - acc: 0.9643 - val_loss: 145.5111 - val_acc: 0.9955\n",
            "Epoch 30/200\n",
            "36000/36000 [==============================] - 3s 88us/sample - loss: 148.1529 - acc: 0.9652 - val_loss: 144.9820 - val_acc: 0.9868\n",
            "Epoch 31/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 147.8278 - acc: 0.9654 - val_loss: 146.0838 - val_acc: 0.9765\n",
            "Epoch 32/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 148.2462 - acc: 0.9647 - val_loss: 145.0131 - val_acc: 0.9755\n",
            "Epoch 33/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 147.5609 - acc: 0.9678 - val_loss: 144.9159 - val_acc: 0.9937\n",
            "Epoch 34/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 147.7605 - acc: 0.9679 - val_loss: 146.1394 - val_acc: 0.9918\n",
            "Epoch 35/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 147.6505 - acc: 0.9675 - val_loss: 145.7956 - val_acc: 0.9883\n",
            "Epoch 36/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 147.4595 - acc: 0.9669 - val_loss: 144.8404 - val_acc: 0.9970\n",
            "Epoch 37/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 147.6814 - acc: 0.9677 - val_loss: 145.3582 - val_acc: 0.9402\n",
            "Epoch 38/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 147.4689 - acc: 0.9681 - val_loss: 146.7004 - val_acc: 0.9847\n",
            "Epoch 39/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 147.3988 - acc: 0.9682 - val_loss: 144.5963 - val_acc: 0.9190\n",
            "Epoch 40/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 147.5210 - acc: 0.9678 - val_loss: 144.9460 - val_acc: 0.9955\n",
            "Epoch 41/200\n",
            "36000/36000 [==============================] - 3s 91us/sample - loss: 147.7311 - acc: 0.9668 - val_loss: 144.2262 - val_acc: 0.9923\n",
            "Epoch 42/200\n",
            "36000/36000 [==============================] - 3s 91us/sample - loss: 147.5210 - acc: 0.9668 - val_loss: 145.4333 - val_acc: 0.9870\n",
            "Epoch 43/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 147.4174 - acc: 0.9682 - val_loss: 145.0669 - val_acc: 0.9915\n",
            "Epoch 44/200\n",
            "36000/36000 [==============================] - 3s 88us/sample - loss: 147.2282 - acc: 0.9674 - val_loss: 146.0622 - val_acc: 0.9660\n",
            "Epoch 45/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 147.8533 - acc: 0.9656 - val_loss: 146.0325 - val_acc: 0.9375\n",
            "Epoch 46/200\n",
            "36000/36000 [==============================] - 3s 91us/sample - loss: 147.6873 - acc: 0.9667 - val_loss: 145.5687 - val_acc: 0.9852\n",
            "Epoch 47/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 147.6103 - acc: 0.9685 - val_loss: 144.3612 - val_acc: 0.9900\n",
            "Epoch 48/200\n",
            "36000/36000 [==============================] - 3s 92us/sample - loss: 147.6350 - acc: 0.9679 - val_loss: 144.4745 - val_acc: 0.9718\n",
            "Epoch 49/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 147.4833 - acc: 0.9697 - val_loss: 144.5028 - val_acc: 0.9465\n",
            "Epoch 50/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 147.5267 - acc: 0.9659 - val_loss: 146.1150 - val_acc: 0.9948\n",
            "Epoch 51/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 147.5004 - acc: 0.9707 - val_loss: 143.6824 - val_acc: 0.9730\n",
            "Epoch 52/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 147.0999 - acc: 0.9693 - val_loss: 144.9633 - val_acc: 0.9775\n",
            "Epoch 53/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 147.1679 - acc: 0.9713 - val_loss: 146.3879 - val_acc: 0.9937\n",
            "Epoch 54/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 147.2008 - acc: 0.9706 - val_loss: 144.0640 - val_acc: 0.9800\n",
            "Epoch 55/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 147.5349 - acc: 0.9703 - val_loss: 144.5447 - val_acc: 0.9735\n",
            "Epoch 56/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 147.1576 - acc: 0.9695 - val_loss: 147.6697 - val_acc: 0.9970\n",
            "Epoch 57/200\n",
            "36000/36000 [==============================] - 3s 88us/sample - loss: 147.0304 - acc: 0.9703 - val_loss: 145.1029 - val_acc: 0.9635\n",
            "Epoch 58/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 147.0652 - acc: 0.9719 - val_loss: 146.1985 - val_acc: 0.9695\n",
            "Epoch 59/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 147.4360 - acc: 0.9693 - val_loss: 145.2959 - val_acc: 0.9820\n",
            "Epoch 60/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 147.0479 - acc: 0.9711 - val_loss: 144.1795 - val_acc: 0.9805\n",
            "Epoch 61/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 147.3399 - acc: 0.9698 - val_loss: 145.4771 - val_acc: 0.9812\n",
            "Epoch 62/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 146.7909 - acc: 0.9691 - val_loss: 144.0884 - val_acc: 0.9595\n",
            "Epoch 63/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 146.9506 - acc: 0.9701 - val_loss: 145.2543 - val_acc: 0.9845\n",
            "Epoch 64/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 146.7991 - acc: 0.9662 - val_loss: 145.0164 - val_acc: 0.9165\n",
            "Epoch 65/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 146.6283 - acc: 0.9672 - val_loss: 143.9086 - val_acc: 0.9718\n",
            "Epoch 66/200\n",
            "36000/36000 [==============================] - 3s 91us/sample - loss: 146.8552 - acc: 0.9651 - val_loss: 143.6734 - val_acc: 0.9513\n",
            "Epoch 67/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 146.8141 - acc: 0.9666 - val_loss: 144.9111 - val_acc: 0.9455\n",
            "Epoch 68/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 146.6660 - acc: 0.9644 - val_loss: 144.6378 - val_acc: 0.9532\n",
            "Epoch 69/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 146.5746 - acc: 0.9665 - val_loss: 147.5138 - val_acc: 0.9628\n",
            "Epoch 70/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 146.7482 - acc: 0.9631 - val_loss: 143.4843 - val_acc: 0.9258\n",
            "Epoch 71/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 146.7242 - acc: 0.9625 - val_loss: 143.9817 - val_acc: 0.9778\n",
            "Epoch 72/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 146.7316 - acc: 0.9641 - val_loss: 145.0061 - val_acc: 0.9570\n",
            "Epoch 73/200\n",
            "36000/36000 [==============================] - 3s 88us/sample - loss: 146.4509 - acc: 0.9654 - val_loss: 144.4814 - val_acc: 0.9740\n",
            "Epoch 74/200\n",
            "36000/36000 [==============================] - 3s 88us/sample - loss: 146.7032 - acc: 0.9636 - val_loss: 143.3631 - val_acc: 0.9870\n",
            "Epoch 75/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 146.7318 - acc: 0.9627 - val_loss: 143.9332 - val_acc: 0.9695\n",
            "Epoch 76/200\n",
            "36000/36000 [==============================] - 3s 88us/sample - loss: 146.5642 - acc: 0.9633 - val_loss: 143.4147 - val_acc: 0.9737\n",
            "Epoch 77/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 146.5727 - acc: 0.9622 - val_loss: 143.9531 - val_acc: 0.9268\n",
            "Epoch 78/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 146.3202 - acc: 0.9627 - val_loss: 143.6913 - val_acc: 0.9912\n",
            "Epoch 79/200\n",
            "36000/36000 [==============================] - 3s 94us/sample - loss: 146.4100 - acc: 0.9629 - val_loss: 143.3833 - val_acc: 0.9745\n",
            "Epoch 80/200\n",
            "36000/36000 [==============================] - 3s 97us/sample - loss: 146.4950 - acc: 0.9628 - val_loss: 144.0610 - val_acc: 0.9685\n",
            "Epoch 81/200\n",
            "36000/36000 [==============================] - 3s 96us/sample - loss: 146.5751 - acc: 0.9602 - val_loss: 144.9839 - val_acc: 0.9762\n",
            "Epoch 82/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 146.3388 - acc: 0.9595 - val_loss: 146.4521 - val_acc: 0.9942\n",
            "Epoch 83/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 146.2541 - acc: 0.9606 - val_loss: 145.7821 - val_acc: 0.9822\n",
            "Epoch 84/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 146.2970 - acc: 0.9621 - val_loss: 143.4181 - val_acc: 0.9668\n",
            "Epoch 85/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 146.3603 - acc: 0.9590 - val_loss: 142.9827 - val_acc: 0.9768\n",
            "Epoch 86/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 146.3940 - acc: 0.9601 - val_loss: 143.5822 - val_acc: 0.9408\n",
            "Epoch 87/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 146.3915 - acc: 0.9600 - val_loss: 143.8346 - val_acc: 0.9290\n",
            "Epoch 88/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 146.1278 - acc: 0.9607 - val_loss: 145.2553 - val_acc: 0.9252\n",
            "Epoch 89/200\n",
            "36000/36000 [==============================] - 3s 88us/sample - loss: 146.3129 - acc: 0.9579 - val_loss: 142.3391 - val_acc: 0.9532\n",
            "Epoch 90/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 146.0865 - acc: 0.9597 - val_loss: 144.2153 - val_acc: 0.9818\n",
            "Epoch 91/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 146.1899 - acc: 0.9575 - val_loss: 144.0164 - val_acc: 0.9670\n",
            "Epoch 92/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 146.1141 - acc: 0.9565 - val_loss: 142.5587 - val_acc: 0.8880\n",
            "Epoch 93/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 146.2427 - acc: 0.9579 - val_loss: 146.6651 - val_acc: 0.9710\n",
            "Epoch 94/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 145.9659 - acc: 0.9559 - val_loss: 148.7858 - val_acc: 0.9810\n",
            "Epoch 95/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 146.1200 - acc: 0.9515 - val_loss: 142.2505 - val_acc: 0.9095\n",
            "Epoch 96/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 145.6911 - acc: 0.9515 - val_loss: 143.8655 - val_acc: 0.9585\n",
            "Epoch 97/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 145.8014 - acc: 0.9500 - val_loss: 143.2765 - val_acc: 0.8817\n",
            "Epoch 98/200\n",
            "36000/36000 [==============================] - 3s 91us/sample - loss: 145.9559 - acc: 0.9499 - val_loss: 143.6894 - val_acc: 0.9737\n",
            "Epoch 99/200\n",
            "36000/36000 [==============================] - 3s 91us/sample - loss: 146.0582 - acc: 0.9484 - val_loss: 143.6478 - val_acc: 0.9237\n",
            "Epoch 100/200\n",
            "36000/36000 [==============================] - 3s 91us/sample - loss: 145.5656 - acc: 0.9436 - val_loss: 143.7082 - val_acc: 0.9222\n",
            "Epoch 101/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 145.4926 - acc: 0.9429 - val_loss: 144.0679 - val_acc: 0.9755\n",
            "Epoch 102/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 145.6273 - acc: 0.9464 - val_loss: 144.5521 - val_acc: 0.9287\n",
            "Epoch 103/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 145.4764 - acc: 0.9432 - val_loss: 143.2047 - val_acc: 0.9425\n",
            "Epoch 104/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 145.7446 - acc: 0.9391 - val_loss: 143.3504 - val_acc: 0.9563\n",
            "Epoch 105/200\n",
            "36000/36000 [==============================] - 3s 88us/sample - loss: 145.5485 - acc: 0.9383 - val_loss: 142.6595 - val_acc: 0.9728\n",
            "Epoch 106/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 145.5361 - acc: 0.9426 - val_loss: 143.2424 - val_acc: 0.9655\n",
            "Epoch 107/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 145.7426 - acc: 0.9485 - val_loss: 144.8324 - val_acc: 0.9383\n",
            "Epoch 108/200\n",
            "36000/36000 [==============================] - 3s 88us/sample - loss: 145.2676 - acc: 0.9600 - val_loss: 142.0406 - val_acc: 0.9740\n",
            "Epoch 109/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 145.3711 - acc: 0.9612 - val_loss: 141.7075 - val_acc: 0.9737\n",
            "Epoch 110/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 145.3853 - acc: 0.9609 - val_loss: 143.6315 - val_acc: 0.9762\n",
            "Epoch 111/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 145.5008 - acc: 0.9577 - val_loss: 142.6785 - val_acc: 0.9572\n",
            "Epoch 112/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 145.2509 - acc: 0.9596 - val_loss: 144.8201 - val_acc: 0.9515\n",
            "Epoch 113/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 145.5386 - acc: 0.9622 - val_loss: 144.2880 - val_acc: 0.9555\n",
            "Epoch 114/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 145.4809 - acc: 0.9582 - val_loss: 142.9476 - val_acc: 0.9902\n",
            "Epoch 115/200\n",
            "36000/36000 [==============================] - 3s 88us/sample - loss: 145.4298 - acc: 0.9603 - val_loss: 144.1754 - val_acc: 0.9875\n",
            "Epoch 116/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 145.2419 - acc: 0.9594 - val_loss: 141.1772 - val_acc: 0.9312\n",
            "Epoch 117/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 145.1085 - acc: 0.9634 - val_loss: 142.1497 - val_acc: 0.9695\n",
            "Epoch 118/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 144.9792 - acc: 0.9620 - val_loss: 144.1709 - val_acc: 0.9622\n",
            "Epoch 119/200\n",
            "36000/36000 [==============================] - 3s 91us/sample - loss: 145.2684 - acc: 0.9609 - val_loss: 142.8669 - val_acc: 0.9262\n",
            "Epoch 120/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 145.2290 - acc: 0.9593 - val_loss: 141.9226 - val_acc: 0.9492\n",
            "Epoch 121/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 145.1877 - acc: 0.9620 - val_loss: 143.9018 - val_acc: 0.9728\n",
            "Epoch 122/200\n",
            "36000/36000 [==============================] - 3s 91us/sample - loss: 145.3000 - acc: 0.9553 - val_loss: 141.8471 - val_acc: 0.8565\n",
            "Epoch 123/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 144.8733 - acc: 0.9607 - val_loss: 141.7703 - val_acc: 0.9783\n",
            "Epoch 124/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 145.0793 - acc: 0.9602 - val_loss: 142.3774 - val_acc: 0.9168\n",
            "Epoch 125/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 145.4027 - acc: 0.9611 - val_loss: 142.6054 - val_acc: 0.9950\n",
            "Epoch 126/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 144.8834 - acc: 0.9600 - val_loss: 142.5776 - val_acc: 0.9068\n",
            "Epoch 127/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 144.6231 - acc: 0.9579 - val_loss: 141.6782 - val_acc: 0.9732\n",
            "Epoch 128/200\n",
            "36000/36000 [==============================] - 3s 87us/sample - loss: 145.0944 - acc: 0.9593 - val_loss: 142.2103 - val_acc: 0.9795\n",
            "Epoch 129/200\n",
            "36000/36000 [==============================] - 3s 87us/sample - loss: 144.7637 - acc: 0.9572 - val_loss: 145.3642 - val_acc: 0.9445\n",
            "Epoch 130/200\n",
            "36000/36000 [==============================] - 3s 87us/sample - loss: 144.8837 - acc: 0.9624 - val_loss: 142.3544 - val_acc: 0.9118\n",
            "Epoch 131/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 144.3792 - acc: 0.9599 - val_loss: 142.2162 - val_acc: 0.9693\n",
            "Epoch 132/200\n",
            "36000/36000 [==============================] - 3s 91us/sample - loss: 144.6477 - acc: 0.9587 - val_loss: 145.9830 - val_acc: 0.9600\n",
            "Epoch 133/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 144.9695 - acc: 0.9547 - val_loss: 141.5957 - val_acc: 0.9785\n",
            "Epoch 134/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 144.5438 - acc: 0.9584 - val_loss: 142.2655 - val_acc: 0.9007\n",
            "Epoch 135/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 144.8903 - acc: 0.9581 - val_loss: 143.0591 - val_acc: 0.9933\n",
            "Epoch 136/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 144.6821 - acc: 0.9573 - val_loss: 142.7609 - val_acc: 0.9745\n",
            "Epoch 137/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 144.6445 - acc: 0.9590 - val_loss: 142.8175 - val_acc: 0.9737\n",
            "Epoch 138/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 144.5594 - acc: 0.9597 - val_loss: 141.9389 - val_acc: 0.9427\n",
            "Epoch 139/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 144.6969 - acc: 0.9617 - val_loss: 141.9979 - val_acc: 0.9308\n",
            "Epoch 140/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 144.4751 - acc: 0.9582 - val_loss: 141.3177 - val_acc: 0.9298\n",
            "Epoch 141/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 144.6429 - acc: 0.9580 - val_loss: 142.6094 - val_acc: 0.9715\n",
            "Epoch 142/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 144.7563 - acc: 0.9589 - val_loss: 142.5168 - val_acc: 0.9690\n",
            "Epoch 143/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 144.0678 - acc: 0.9582 - val_loss: 143.0316 - val_acc: 0.9700\n",
            "Epoch 144/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 144.4418 - acc: 0.9594 - val_loss: 142.3370 - val_acc: 0.9655\n",
            "Epoch 145/200\n",
            "36000/36000 [==============================] - 3s 91us/sample - loss: 144.4314 - acc: 0.9584 - val_loss: 141.9831 - val_acc: 0.9517\n",
            "Epoch 146/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 144.3916 - acc: 0.9584 - val_loss: 141.1880 - val_acc: 0.9595\n",
            "Epoch 147/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 144.5454 - acc: 0.9592 - val_loss: 145.1576 - val_acc: 0.9563\n",
            "Epoch 148/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 144.3925 - acc: 0.9592 - val_loss: 140.7730 - val_acc: 0.9310\n",
            "Epoch 149/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 144.3702 - acc: 0.9588 - val_loss: 141.4470 - val_acc: 0.9563\n",
            "Epoch 150/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 144.5241 - acc: 0.9530 - val_loss: 141.6006 - val_acc: 0.9315\n",
            "Epoch 151/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 143.9960 - acc: 0.9582 - val_loss: 140.5009 - val_acc: 0.9732\n",
            "Epoch 152/200\n",
            "36000/36000 [==============================] - 3s 87us/sample - loss: 144.6241 - acc: 0.9578 - val_loss: 146.5117 - val_acc: 0.9805\n",
            "Epoch 153/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 144.3607 - acc: 0.9581 - val_loss: 141.0526 - val_acc: 0.9588\n",
            "Epoch 154/200\n",
            "36000/36000 [==============================] - 3s 88us/sample - loss: 144.3179 - acc: 0.9586 - val_loss: 141.1292 - val_acc: 0.9035\n",
            "Epoch 155/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 144.5049 - acc: 0.9583 - val_loss: 141.6740 - val_acc: 0.9877\n",
            "Epoch 156/200\n",
            "36000/36000 [==============================] - 3s 87us/sample - loss: 144.4813 - acc: 0.9594 - val_loss: 141.3569 - val_acc: 0.9193\n",
            "Epoch 157/200\n",
            "36000/36000 [==============================] - 3s 86us/sample - loss: 143.8187 - acc: 0.9581 - val_loss: 140.4876 - val_acc: 0.9843\n",
            "Epoch 158/200\n",
            "36000/36000 [==============================] - 3s 87us/sample - loss: 144.2889 - acc: 0.9618 - val_loss: 141.6157 - val_acc: 0.9743\n",
            "Epoch 159/200\n",
            "36000/36000 [==============================] - 3s 86us/sample - loss: 144.1095 - acc: 0.9593 - val_loss: 140.3227 - val_acc: 0.9195\n",
            "Epoch 160/200\n",
            "36000/36000 [==============================] - 3s 87us/sample - loss: 144.1897 - acc: 0.9546 - val_loss: 141.3994 - val_acc: 0.9270\n",
            "Epoch 161/200\n",
            "36000/36000 [==============================] - 3s 87us/sample - loss: 144.0615 - acc: 0.9586 - val_loss: 141.7117 - val_acc: 0.9668\n",
            "Epoch 162/200\n",
            "36000/36000 [==============================] - 3s 87us/sample - loss: 143.8823 - acc: 0.9539 - val_loss: 142.6385 - val_acc: 0.9337\n",
            "Epoch 163/200\n",
            "36000/36000 [==============================] - 3s 88us/sample - loss: 144.3811 - acc: 0.9557 - val_loss: 142.1886 - val_acc: 0.9553\n",
            "Epoch 164/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 143.8971 - acc: 0.9543 - val_loss: 147.3904 - val_acc: 0.9815\n",
            "Epoch 165/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 143.9493 - acc: 0.9580 - val_loss: 145.5149 - val_acc: 0.9745\n",
            "Epoch 166/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 144.3650 - acc: 0.9572 - val_loss: 147.3559 - val_acc: 0.8903\n",
            "Epoch 167/200\n",
            "36000/36000 [==============================] - 3s 87us/sample - loss: 143.7929 - acc: 0.9537 - val_loss: 141.3960 - val_acc: 0.9862\n",
            "Epoch 168/200\n",
            "36000/36000 [==============================] - 3s 87us/sample - loss: 143.9085 - acc: 0.9534 - val_loss: 142.6336 - val_acc: 0.9780\n",
            "Epoch 169/200\n",
            "36000/36000 [==============================] - 3s 87us/sample - loss: 143.8972 - acc: 0.9539 - val_loss: 145.6644 - val_acc: 0.9825\n",
            "Epoch 170/200\n",
            "36000/36000 [==============================] - 3s 88us/sample - loss: 143.9184 - acc: 0.9522 - val_loss: 144.3834 - val_acc: 0.9735\n",
            "Epoch 171/200\n",
            "36000/36000 [==============================] - 3s 87us/sample - loss: 144.0950 - acc: 0.9540 - val_loss: 140.4971 - val_acc: 0.9665\n",
            "Epoch 172/200\n",
            "36000/36000 [==============================] - 3s 87us/sample - loss: 144.0179 - acc: 0.9540 - val_loss: 140.9513 - val_acc: 0.9317\n",
            "Epoch 173/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 144.0406 - acc: 0.9558 - val_loss: 141.8086 - val_acc: 0.9442\n",
            "Epoch 174/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 144.1240 - acc: 0.9554 - val_loss: 144.2384 - val_acc: 0.9237\n",
            "Epoch 175/200\n",
            "36000/36000 [==============================] - 3s 95us/sample - loss: 143.6934 - acc: 0.9554 - val_loss: 146.0696 - val_acc: 0.9805\n",
            "Epoch 176/200\n",
            "36000/36000 [==============================] - 3s 95us/sample - loss: 143.9285 - acc: 0.9568 - val_loss: 142.2977 - val_acc: 0.8995\n",
            "Epoch 177/200\n",
            "36000/36000 [==============================] - 3s 93us/sample - loss: 143.6959 - acc: 0.9524 - val_loss: 141.7876 - val_acc: 0.9643\n",
            "Epoch 178/200\n",
            "36000/36000 [==============================] - 3s 93us/sample - loss: 143.8104 - acc: 0.9595 - val_loss: 142.3879 - val_acc: 0.9720\n",
            "Epoch 179/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 143.5946 - acc: 0.9584 - val_loss: 141.3710 - val_acc: 0.9675\n",
            "Epoch 180/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 143.8316 - acc: 0.9558 - val_loss: 140.8883 - val_acc: 0.9900\n",
            "Epoch 181/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 143.9503 - acc: 0.9605 - val_loss: 140.7796 - val_acc: 0.9170\n",
            "Epoch 182/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 143.8864 - acc: 0.9596 - val_loss: 141.4535 - val_acc: 0.9845\n",
            "Epoch 183/200\n",
            "36000/36000 [==============================] - 3s 88us/sample - loss: 143.6123 - acc: 0.9578 - val_loss: 140.9844 - val_acc: 0.9910\n",
            "Epoch 184/200\n",
            "36000/36000 [==============================] - 3s 88us/sample - loss: 143.8799 - acc: 0.9601 - val_loss: 140.7586 - val_acc: 0.9283\n",
            "Epoch 185/200\n",
            "36000/36000 [==============================] - 3s 87us/sample - loss: 143.5461 - acc: 0.9575 - val_loss: 146.4318 - val_acc: 0.9840\n",
            "Epoch 186/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 144.1225 - acc: 0.9592 - val_loss: 142.3741 - val_acc: 0.9528\n",
            "Epoch 187/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 143.4559 - acc: 0.9593 - val_loss: 141.3488 - val_acc: 0.9532\n",
            "Epoch 188/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 143.7943 - acc: 0.9578 - val_loss: 140.7163 - val_acc: 0.9885\n",
            "Epoch 189/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 144.0011 - acc: 0.9590 - val_loss: 142.7249 - val_acc: 0.9545\n",
            "Epoch 190/200\n",
            "36000/36000 [==============================] - 3s 88us/sample - loss: 143.7720 - acc: 0.9547 - val_loss: 139.9283 - val_acc: 0.9143\n",
            "Epoch 191/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 143.4597 - acc: 0.9574 - val_loss: 143.5830 - val_acc: 0.9575\n",
            "Epoch 192/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 143.5976 - acc: 0.9560 - val_loss: 139.5125 - val_acc: 0.9595\n",
            "Epoch 193/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 143.5720 - acc: 0.9543 - val_loss: 140.7899 - val_acc: 0.9492\n",
            "Epoch 194/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 143.6166 - acc: 0.9556 - val_loss: 140.2170 - val_acc: 0.9902\n",
            "Epoch 195/200\n",
            "36000/36000 [==============================] - 3s 89us/sample - loss: 143.8817 - acc: 0.9560 - val_loss: 141.1247 - val_acc: 0.9787\n",
            "Epoch 196/200\n",
            "36000/36000 [==============================] - 3s 90us/sample - loss: 143.2960 - acc: 0.9553 - val_loss: 139.9432 - val_acc: 0.9743\n",
            "Epoch 197/200\n",
            "36000/36000 [==============================] - 3s 95us/sample - loss: 143.5580 - acc: 0.9535 - val_loss: 140.5970 - val_acc: 0.9118\n",
            "Epoch 198/200\n",
            "36000/36000 [==============================] - 3s 93us/sample - loss: 143.4166 - acc: 0.9548 - val_loss: 143.6399 - val_acc: 0.9693\n",
            "Epoch 199/200\n",
            "36000/36000 [==============================] - 3s 91us/sample - loss: 143.3729 - acc: 0.9535 - val_loss: 140.3905 - val_acc: 0.9140\n",
            "Epoch 200/200\n",
            "36000/36000 [==============================] - 3s 92us/sample - loss: 143.4689 - acc: 0.9545 - val_loss: 141.4782 - val_acc: 0.9270\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb06c59bd68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYJF-hBOmmoG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights(\"model-1.hdf5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clBcmfaLxi86",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ds = pd.read_csv(\"test.csv\")\n",
        "\n",
        "#dataset = df.values\n",
        "ds = ds.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "to_drop = ['ID','DateTime','HiDir','WindDir']\n",
        "ds = ds.drop(columns = to_drop)\n",
        "\n",
        "feature_cols = ['TempOut','HiTemp','LowTemp','OutHum','DewPt','WindSpeed',\n",
        "               'WindRun','HiSpeed','WindChill','HeatIndex','THWIndex','Bar',\n",
        "               'Rain','RainRate','HeatDD','CoolDD','InTemp','InDew','InDew',\n",
        "               'InHeat','InEMC','InAirDensity','WindSamp','WindTx','ISSRecpt',\n",
        "               'ArcInt']\n",
        "\n",
        "X_test = ds[feature_cols].values\n",
        "\n",
        "model.load_weights(\"model-1.hdf5\")\n",
        "\n",
        "predicted = model.predict(X_test)\n",
        "\n",
        "vals = np.round(predicted)\n",
        "range =np.arange(40001,62447)\n",
        "\n",
        "pa = vals[:,0].tolist()\n",
        "pb = vals[:,1].tolist()\n",
        "pc = vals[:,2].tolist()\n",
        "pda = vals[:,3].tolist()\n",
        "pe = vals[:,4].tolist()\n",
        "pf = vals[:,5].tolist()\n",
        "pg = vals[:,6].tolist()\n",
        "\n",
        "with open(\"output-model-1.csv\",\"w\") as f:\n",
        "  f.write(\"ID,PA,PB,PC,PD,PE,PF,PG\\n\")\n",
        "  for x,y1,y2,y3,y4,y5,y6,y7 in zip(range,pa,pb,pc,pda,pe,pf,pg):\n",
        "    f.write(\"PR{},{},{},{},{},{},{},{}\\n\".format(x,y1,y2,y3,y4,y5,y6,y7))\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WiUnh7CEFANl",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}